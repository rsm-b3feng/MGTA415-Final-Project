{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3d61f7-0b3b-44bb-8e18-123f49be087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, preprocess_string, stem_text\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c464f68-7463-46bf-9c9c-2581335e3434",
   "metadata": {
    "papermill": {
     "duration": 0.011261,
     "end_time": "2023-01-18T08:29:14.794210",
     "exception": false,
     "start_time": "2023-01-18T08:29:14.782949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0423509-9bed-47d3-ba4c-f8629ff16557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "essays = pd.read_csv('essays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f8ac5-a9ea-4bfd-b7e4-db4f41556f73",
   "metadata": {},
   "source": [
    "essay0- My self summary \\\n",
    "essay1- What I’m doing with my life \\\n",
    "essay2- I’m really good at \\\n",
    "essay3- The first thing people usually notice about me \\\n",
    "essay4- Favorite books, movies, show, music, and food \\\n",
    "essay5- The six things I could never do without \\\n",
    "essay6- I spend a lot of time thinking about \\\n",
    "essay7- On a typical Friday night I am \\\n",
    "essay8- The most private thing I am willing to admit \\\n",
    "essay9- You should message me if..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2db6e9-e247-4ece-a870-d05e853aedf2",
   "metadata": {
    "papermill": {
     "duration": 0.142702,
     "end_time": "2023-01-18T08:29:15.068584",
     "exception": false,
     "start_time": "2023-01-18T08:29:14.925882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "essays = essays.fillna('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "851af617-a55b-4644-a661-d1fc2ab17076",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame()\n",
    "clean_df[\"id\"] = essays.index\n",
    "# set 'essay0','essay1','essay2','essay5','essay7' as description\n",
    "des_col = ['essay0','essay1','essay2','essay5','essay7']\n",
    "clean_df['describe'] = essays[des_col].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "# set 'essay9' as expectation\n",
    "clean_df['expect'] = essays[essays.columns[-1:]]\n",
    "\n",
    "# drop na value\n",
    "clean_df = clean_df[-(clean_df['describe'] == \"........\")]\n",
    "clean_df = clean_df[-(clean_df['expect'] == \".\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a7f84-3d65-43ef-96cf-4c66e3fdd44b",
   "metadata": {
    "papermill": {
     "duration": 0.011307,
     "end_time": "2023-01-18T08:29:15.124854",
     "exception": false,
     "start_time": "2023-01-18T08:29:15.113547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data: The dataset comprises 10 essays (essay0 to essay9), totaling 59,946 rows.\n",
    "\n",
    "Feature Selection (Column Processing): \n",
    "\n",
    "Since interests are used as classification criteria, we concatenated essay0, essay1, essay2, essay5, and essay7 to form a comprehensive user description while excluding essay4 (Favorite books, movies, shows, music, and food) to prevent potential noise caused by the diverse range of interests listed in this section. Additionally, essay9 was selected to represent each user’s expectations for a potential match. The user ID was also retained.\n",
    "\n",
    "Data Cleaning (Row Processing): \n",
    "\n",
    "Entries with missing values in all five descriptive essays (essay0, essay1, essay2, essay5, essay7) or in essay9 were removed, resulting in 47,335 remaining rows. To test different methods on a smaller scale, a random sample of 3,000 users was selected using a sampling approach.\n",
    "\n",
    "Data Transformation: \n",
    "\n",
    "We applied the ‘en_core_web_sm’ model from SpaCy to process the user descriptions (describe), performing tokenization, punctuation and stop-word removal, and lemmatization, ultimately generating describe_ls. The same preprocessing steps were applied to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fea0ff-b979-45dc-b29d-fdbcccff8076",
   "metadata": {
    "papermill": {
     "duration": 4.739625,
     "end_time": "2023-01-18T08:29:19.875909",
     "exception": false,
     "start_time": "2023-01-18T08:29:15.136284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aggregating user-generated essays\n",
    "sample = clean_df.sample(n=3000, replace=False, random_state=42).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf48542-9f87-45a4-bd57-13ef02fb8f79",
   "metadata": {
    "papermill": {
     "duration": 9.38113,
     "end_time": "2023-01-18T08:29:29.269144",
     "exception": false,
     "start_time": "2023-01-18T08:29:19.888014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process_text(text, extra_list):\n",
    "\n",
    "    # Load stopwords\n",
    "    all_stopwords = nlp.Defaults.stop_words.union(set(extra_list))\n",
    "    \n",
    "    # Tokenization using Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Process tokens\n",
    "    tokens_clean = [\n",
    "        (token.lemma_[:-4] if token.lemma_.endswith('.<br') else token.lemma_)\n",
    "        for token in doc\n",
    "        if token.text.lower() not in all_stopwords and not token.is_punct\n",
    "    ]\n",
    "    \n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba991dc-ce31-471b-855c-0fd9ebfa9907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:12<00:00, 41.56it/s]\n",
      "100%|██████████| 3000/3000 [00:14<00:00, 213.81it/s]\n"
     ]
    }
   ],
   "source": [
    "extra_list = ['\\n','<','>','br','.','<br']\n",
    "\n",
    "sample['describe_ls'] = sample['describe'].progress_apply(lambda x: process_text(x, extra_list))\n",
    "sample['expect_ls'] = sample['expect'].progress_apply(lambda x: process_text(x, extra_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac71d06-0fef-4916-abb2-3def93006710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.to_csv('sample_less.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00821478-e32c-4858-882f-56f08ff3f1fa",
   "metadata": {
    "papermill": {
     "duration": 0.011476,
     "end_time": "2023-01-18T08:29:33.845217",
     "exception": false,
     "start_time": "2023-01-18T08:29:33.833741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Contextualized Weak Supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76e988-274c-4017-b40f-d25598e8849e",
   "metadata": {},
   "source": [
    "Logistics:\n",
    "\n",
    "We first divide both the user descriptions (describe) and expectations (expect) into 10 groups. Each user will be matched with others whose describe label aligns with their expect label.\n",
    "\n",
    "Method: \n",
    "\n",
    "Due to the lack of labeled data, we employ a Minimally Supervised approach. Specifically, we manually define 10 categories, each associated with either 3 or 10 seed words, using only the textual content of the documents. Given that state-of-the-art methods such as WeSTClass and ConWea require significant computational resources, we instead adopt a structural approximation. We determine category assignment by comparing the similarity between seed words and the text within each category.\n",
    "\n",
    "To ensure that the categories are both mutually independent and representative, as well as that the seed words within each category are meaningful, we generate them using the following structured dialogue with ChatGPT: https://chatgpt.com/share/e/67d609c4-6ed4-800e-ad6b-921d13f6250d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe7e36f-4358-4b09-ab87-a586ef3afec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pd.read_csv('sample_less.csv')\n",
    "# sample['describe_ls'] = sample['describe_ls'].apply(lambda x: x[2:-2].split(\"', '\"))\n",
    "# sample['expect_ls'] = sample['expect_ls'].apply(lambda x: x[2:-2].split(\"', '\"))\n",
    "# del sample['Unnamed: 0']\n",
    "# del sample['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3839e8-32c5-4c4c-bd97-5fd89c235f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv('dating_categories.csv')\n",
    "cat_df['Seed Words'] = cat_df['Seed Words'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa8d747-040b-4e2f-8ffd-19fef73c5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df_10 = pd.read_csv('dating_categories_updated.csv')\n",
    "cat_df_10['Seed Words'] = cat_df_10['Seed Words'].apply(lambda x: x[2:-2].split(\"', '\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b6dcc-6849-47b8-b890-92685e5efacc",
   "metadata": {},
   "source": [
    "## Algorithm 1: String Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24daeb7c-e351-4edc-99fd-7b11b4702ebd",
   "metadata": {},
   "source": [
    "We assume that if certain keywords appear frequently in a document, the document is likely to belong to the corresponding category. Based on this assumption, we calculate the frequency of seed words from different categories appearing in each user’s describe section and assign the label of the category with the highest frequency (Figure 1).\n",
    "\n",
    "Initially, we determined labels solely based on the total frequency of all seed words. However, this approach presented a limitation: If a specific seed word (e.g., love) is widely used across different contexts, many users may be incorrectly classified into that category.\n",
    "\n",
    "To mitigate this issue, we refined our method by first assessing the coverage of seed words in the text before considering their frequency. Specifically, we first count the number of unique seed words from each category that appear in a given text. If multiple categories contain the same number of unique seed words, we then compare the total occurrences of all three seed words within the text. This refined approach effectively prevents overgeneralization into a single dominant category and provides a more accurate representation of users’ actual preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2313c811-4433-4fb9-ba25-341a9ba2fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_category_with_coverage(desc_list, cat_df):\n",
    "    best_category = None\n",
    "    word_counts = Counter(desc_list)\n",
    "    category_scores = {}\n",
    "\n",
    "    for _, row in cat_df.iterrows():\n",
    "        category = row['Category']\n",
    "        seed_words = set(row['Seed Words'])  # Convert to set for uniqueness\n",
    "\n",
    "        # Count how many unique seed words are covered\n",
    "        unique_matches = sum(1 for word in seed_words if word in word_counts)\n",
    "        # Compute total occurrences of all matched seed words\n",
    "        total_match_count = sum(word_counts[word] for word in seed_words if word in word_counts)\n",
    "\n",
    "        category_scores[category] = (unique_matches, total_match_count)\n",
    "\n",
    "    # Determine the best category based on unique matches first, then frequency\n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)\n",
    "\n",
    "    if len(sorted_categories) == 1 or (sorted_categories[0][1] > sorted_categories[1][1]):\n",
    "        best_category = sorted_categories[0][0]\n",
    "\n",
    "    return best_category\n",
    "\n",
    "def ar1(sample_df, cat_df):\n",
    "    \n",
    "    cat_fq_des = sample_df['describe_ls'].progress_apply(lambda x: assign_category_with_coverage(x, cat_df))\n",
    "    car_fq_exp = sample_df['expect_ls'].progress_apply(lambda x: assign_category_with_coverage(x, cat_df))\n",
    "\n",
    "    return cat_fq_des,car_fq_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6ea04f-7b15-4ae0-8791-9549d41183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fq_des,cat_fq_exp = ar1(sample, cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8c4adc-d648-4cc7-ad5a-4d53371e1046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1075, 2322)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fq_des.isna().sum(),cat_fq_exp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9589a073-71cc-4178-9acc-d6c19bb9a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fq_des_10,cat_fq_exp_10 = ar1(sample, cat_df_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6ebb59-1fc9-4550-a99e-1d314dcb7106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845, 2117)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fq_des_10.isna().sum(),cat_fq_exp_10.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ea1a1-5d48-4f5f-8a0f-c97162770c6e",
   "metadata": {},
   "source": [
    "This method has a limitation: the set of seed words is finite. For instance, the seed words for the Adventurous Outdoorsy category are travel, hike, and adventure. However, if a text describes outdoor activities such as camping, mountain climbing, or skiing, it may not be classified into this category due to the absence of these specific keywords.\n",
    "\n",
    "As a result, when the number of seed words is set to three, one-third of the describe entries and two-thirds of the expect entries cannot be classified. Expanding the seed word list to ten reduces the number of unclassified instances, but still, 28% of the describe entries and two-thirds of the expect entries remain unclassified.\n",
    "\n",
    "The higher proportion of missing classifications in expect than describe is primarily due to the fact that most users provide only a short sentence for their expectations, whereas their self-descriptions tend to be more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e86b08-22f9-4d99-987f-a9deb9caa690",
   "metadata": {},
   "source": [
    "## Algorithm 2: Similarity Comparison on the entire document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa31d4f-f51d-4dfe-83e8-157f97c5b28e",
   "metadata": {},
   "source": [
    "To measure similarity, we use cosine similarity instead of Euclidean distance. This choice is based on our observation that the seed words from different categories have significant differences in vector magnitudes, which could result in most users being classified into a single dominant category. Cosine similarity, by ignoring vector magnitude and focusing on angular difference, provides a more balanced classification approach.\n",
    "\n",
    "We first vectorize the describe text using the word2vec-google-news-300 model, obtaining a matrix of size (number of words in the document × 300). We then compute the column-wise mean of this matrix to derive a single 1 × 300 vector representation for the entire document.\n",
    "\n",
    "Next, we apply the same word embedding method to the seed words of each category. For a category with three seed words, we obtain a 3 × 300 matrix.\n",
    "\n",
    "We then compute the cosine similarity between the document vector and each of the three seed word vectors within a category. The category is selected based on the following rules:\n",
    "\t1.\tCompute the average cosine similarity of the three seed words for each category and select the category with the highest value.\n",
    "\t2.\tIf the highest average similarity among all categories is ≤ 0, then select the category with the highest individual similarity score.\n",
    "\t3.\tIf the highest individual similarity score among all categories is still ≤ 0, return None (unclassified).\n",
    "\n",
    "The same classification procedure is applied to the expect text.(figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840e58f8-0810-4476-90a1-d31198686a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model_google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11817961-c698-4a92-9caf-8203479d882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(doc,wv,len=300):\n",
    "    vecs = []\n",
    "    for token in doc:\n",
    "        try:\n",
    "            vecs.append(wv[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if vecs == []:\n",
    "        return np.zeros(len)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def assign_category_by_similarity(desc_vec, cat_df, row_name='seed_vec'):\n",
    "    \n",
    "    category_scores = {}\n",
    "    max_value_per_category = {}\n",
    "    # Iterate through each category and compute similarity scores\n",
    "    for _, row in cat_df.iterrows():\n",
    "        category = row['Category']\n",
    "        seed_vecs = row[row_name]\n",
    "        \n",
    "        # Compute cosine similarity for each seed word\n",
    "        desc_vec_norm = np.linalg.norm(desc_vec) + 1e-9  \n",
    "        seed_vecs_norm = np.linalg.norm(seed_vecs, axis=1) + 1e-9\n",
    "\n",
    "        similarities = np.dot(seed_vecs, desc_vec) / (seed_vecs_norm * desc_vec_norm)\n",
    "        \n",
    "        # Compute the average and max similarity score\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        max_similarity = np.max(similarities)\n",
    "\n",
    "        category_scores[category] = avg_similarity\n",
    "        max_value_per_category[category] = max_similarity\n",
    "\n",
    "    # Find the best category based on average similarity\n",
    "    best_category = max(category_scores, key=category_scores.get)\n",
    "    best_score = category_scores[best_category]\n",
    "\n",
    "    if best_score > 0:\n",
    "        return best_category\n",
    "    else:\n",
    "        # If all scores are <= 0, choose based on max individual similarity value\n",
    "        best_category_by_max = max(max_value_per_category, key=max_value_per_category.get)\n",
    "        best_max_value = max_value_per_category[best_category_by_max]\n",
    "        if best_max_value > 0:\n",
    "            return best_category_by_max\n",
    "        else:\n",
    "            return None  # If all values are <= 0\n",
    "\n",
    "\n",
    "\n",
    "def ar2(sample_df,cat_df):\n",
    "    \n",
    "    sample_df['describe_vec'] = sample_df['describe_ls'].progress_apply(lambda x: doc2vec(x,model_google))\n",
    "    sample_df['expect_vec'] = sample_df['expect_ls'].progress_apply(lambda x: doc2vec(x,model_google))\n",
    "    cat_df['seed_vec'] = cat_df['Seed Words'].progress_apply(lambda x: np.array([doc2vec(wd,model_google) for wd in x]))\n",
    "    \n",
    "    cat_sm_des = sample_df['describe_vec'].progress_apply(lambda x: assign_category_by_similarity(x, cat_df))\n",
    "    cat_sm_exp = sample_df['expect_vec'].progress_apply(lambda x: assign_category_by_similarity(x, cat_df))\n",
    "\n",
    "    return cat_sm_des,cat_sm_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9054a62d-4c5b-4fe4-93b1-dc27ab817254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sm_des,cat_sm_exp = ar2(sample, cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee9a2dd4-49b5-4513-b8b0-e0f2346d9fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_sm_des.isna().sum(),cat_sm_exp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f86679e-6ccf-4706-a116-a672b36fd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sm_des_10,cat_sm_exp_10 = ar2(sample, cat_df_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11b43b-87e5-4b27-8c53-58cd63bf8f28",
   "metadata": {},
   "source": [
    "This method effectively reduces the issue of unclassified instances present in Algorithm 1. However, a small number of descriptions still cannot be classified due to insufficient information.(e.g. someone wrote the expectation as \"at least 5'4\"\") With the three-seed-word setting, the proportion of unclassified entries is 1% for describe and 2% for expect.\n",
    "\n",
    "Additionally, this method has a limitation: averaging the vectors of all words compresses a significant amount of information. This approach assumes that all words contribute equally to the final representation, which is not always realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32c6c7-242d-4085-8518-de01405fdb2b",
   "metadata": {},
   "source": [
    "## Algorithm 3: Similarity Comparison on Key Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51471712-cf31-43f0-ba19-d4c67645b113",
   "metadata": {},
   "source": [
    "To emphasize the contribution of important words, we use TF-IDF to select the top 10 most important words from each describe entry. We set min_df = 2 to ignore words that appear only once, reducing noise caused by typographical errors. For sentences containing fewer than ten words, we select all words with a TF-IDF score > 0.\n",
    "\n",
    "Next, we vectorize all selected keywords using word2vec-google-news-300, resulting in a (number of keywords × 300) matrix. We then compute the cosin similarity between each keyword and each category’s seed words. For each category, we take the highest similarity score among all keyword–seed word pairs as the category’s final score. The category with the highest score is assigned as the label. (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40064ce1-bd55-46bc-ba57-44a0b8627bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(min_df=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(min_df=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(min_df=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2) # 减少误打的字的影响\n",
    "vectorizer.fit([\" \".join(tokens) for tokens in sample['describe_ls']] \n",
    "               + [\" \".join(tokens) for tokens in sample['expect_ls']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ebce181-4aa6-4cd0-8041-d6c8556658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_tfidf_words_single(vectorizer, row_tokens, top_n=10):\n",
    "\n",
    "    if not row_tokens:  # 处理空列表情况\n",
    "        return []\n",
    "\n",
    "    row_text = \" \".join(row_tokens)\n",
    "    row_tfidf = vectorizer.transform([row_text])  # 仅计算当前行的 TF-IDF\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 获取 TF-IDF 词语及其得分\n",
    "    row_tfidf_scores = row_tfidf.toarray().flatten()\n",
    "    non_zero_indices = row_tfidf_scores.nonzero()[0]  # 仅选取 TF-IDF > 0 的词索引\n",
    "\n",
    "    # 处理可能的短行情况\n",
    "    num_top_words = min(top_n, len(non_zero_indices))  # 取最小值，防止超出索引范围\n",
    "    if np.sum(row_tfidf_scores) == 0:\n",
    "        return []\n",
    "    # 按照得分排序，获取重要词语\n",
    "    top_indices = row_tfidf_scores.argsort()[-num_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "def compute_max_cosine_similarity(describe_top_g, seed_vec_g):\n",
    "    \n",
    "    if describe_top_g.shape[0] == 0:\n",
    "        return 0.0  # 如果描述为空，返回0\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    describe_norm = np.linalg.norm(describe_top_g, axis=1, keepdims=True) + 1e-9  # 避免除零\n",
    "    seed_norm = np.linalg.norm(seed_vec_g, axis=1, keepdims=True) + 1e-9\n",
    "\n",
    "    cosine_similarities = np.dot(describe_top_g, seed_vec_g.T) / (describe_norm * seed_norm.T)  # 计算 x*3 矩阵\n",
    "\n",
    "    # 计算最终得分\n",
    "    final_score = np.max(cosine_similarities)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "def similarity(x,cat_df,row_name = 'seed_vec_g'):\n",
    "    category_scores = {}\n",
    "    for i in range(cat_df.shape[0]):\n",
    "        seed_vec_g = cat_df[row_name][i]\n",
    "        cate = cat_df['Category'][i]\n",
    "        category_scores[cate] = compute_max_cosine_similarity(x, seed_vec_g)\n",
    "\n",
    "    best_category = max(category_scores, key=category_scores.get)\n",
    "    best_score = category_scores[best_category]\n",
    "\n",
    "    return best_category\n",
    "\n",
    "def ar3(sample_df,cat_df):\n",
    "    \n",
    "    sample_df['describe_top10'] = sample_df['describe_ls'].progress_apply(lambda x: extract_top_tfidf_words_single(vectorizer,x))\n",
    "    sample_df['expect_top10'] = sample_df['expect_ls'].progress_apply(lambda x: extract_top_tfidf_words_single(vectorizer,x))\n",
    "\n",
    "    sample_df['describe_top_vec'] = sample_df['describe_top10'].progress_apply(lambda x:  np.array([doc2vec(wd,model_google) for wd in x]))\n",
    "    sample_df['expect_top_vec'] = sample_df['expect_top10'].progress_apply(lambda x: np.array([doc2vec(wd,model_google) for wd in x]))\n",
    "    cat_df['seed_vec'] = cat_df['Seed Words'].apply(lambda x: np.array([doc2vec(wd,model_google) for wd in x]))\n",
    "\n",
    "    cat_kw_des = sample_df['describe_top_vec'].progress_apply(lambda x: similarity(x, cat_df,row_name = 'seed_vec'))\n",
    "    cat_kw_exp = sample_df['expect_top_vec'].progress_apply(lambda x: similarity(x, cat_df,row_name = 'seed_vec'))\n",
    "\n",
    "    return cat_kw_des,cat_kw_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ebc8046-3aa9-406f-9a1e-417e88e38b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:07<00:00, 424.45it/s]\n",
      "100%|██████████| 3000/3000 [00:06<00:00, 449.05it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_kw_des,cat_kw_exp = ar3(sample, cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f577446-81a9-4223-b4a2-d093687d37c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_kw_des.isna().sum(),cat_kw_exp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7e7d0a6-eb8e-4549-8f3b-391ee9b124b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:07<00:00, 395.70it/s]\n",
      "100%|██████████| 3000/3000 [00:06<00:00, 452.81it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_kw_des_10,cat_kw_exp_10 = ar3(sample, cat_df_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cfe68-3ada-4145-a844-1beef2518b89",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cb42f-97f3-46ec-ae2f-cb718b97cf6b",
   "metadata": {},
   "source": [
    "Due to the lack of labeled data, we could only randomly sample instances for evaluation. Upon manual inspection, we found that each of the three methods had its own advantages. Therefore, we selected the most frequently assigned category across all six methods as the final label.\n",
    "\n",
    "However, even when combining all six methods, this seed-word-based weakly supervised learning approach still has several limitations.\n",
    "\n",
    "First, all methods assume that each user belongs to a single category, making it difficult to accurately classify individuals with diverse interests. In dating apps, most users tend to present multiple facets of themselves, leading to high variance in our classification results.\n",
    "\n",
    "Second, none of the methods account for negative exclusions. For example:\n",
    "\n",
    "1. If a user’s expect states, “You should message me if you’re not a nerdy guy,” the model would delete the stop word \"not\" and likely classify them under Intellectual/Bookish based on \"nerdy\", which contradicts their actual preference.\n",
    "\n",
    "2. Similarly, if someone’s expect states, “You should message me if you accept an unconventional family and want to have a child,” the presence of keywords like child and family may lead to classification under Family-Oriented. However, this expectation does not align with the typical preferences of users labeled as Family-Oriented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "575c6a06-8be2-4b92-a7f0-b8e11eb56d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['describe'] = sample['describe']\n",
    "result['label_frequence'] = cat_fq_des\n",
    "result['label_frequence_10'] = cat_fq_des_10\n",
    "result['label_similarity'] = cat_sm_des\n",
    "result['label_similarity_10'] = cat_sm_des_10\n",
    "result['label_keywords'] = cat_kw_des\n",
    "result['label_keywords_10'] = cat_kw_des_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f6520c7-3c2a-42a5-97c4-bb6944aca4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_label(row):\n",
    "\n",
    "    label_counts = {}\n",
    "\n",
    "    for col in ['label_frequence', 'label_frequence_10', 'label_similarity', \n",
    "                'label_similarity_10', 'label_keywords', 'label_keywords_10']:\n",
    "        label = row[col]\n",
    "        if pd.notna(label):  # 确保不是 NaN\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "    if not label_counts:\n",
    "        return None  # 如果没有任何 label，返回 None\n",
    "\n",
    "    max_count = max(label_counts.values())\n",
    "    top_labels = [label for label, count in label_counts.items() if count == max_count]\n",
    "\n",
    "    return top_labels[0] if len(top_labels) == 1 else None  # 如果有多个相同最大值的 label，则返回 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f08b804b-c921-48c2-98ca-cd9c768b2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_des_label = result.apply(most_frequent_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8b36e3-f443-46f3-9e9f-735477c44313",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['label_frequence'] = cat_fq_exp\n",
    "result['label_frequence_10'] = cat_fq_exp_10\n",
    "result['label_similarity'] = cat_sm_exp\n",
    "result['label_similarity_10'] = cat_sm_exp_10\n",
    "result['label_keywords'] = cat_kw_exp\n",
    "result['label_keywords_10'] = cat_kw_exp_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f4634f5-3025-4855-8376-e436d82051f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_exp_label = result.apply(most_frequent_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4de37-cf52-4228-8d9c-1a62d8733a9c",
   "metadata": {},
   "source": [
    "# Roll Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "908c1525-6b5f-4474-97e8-38ceece0491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47335/47335 [21:37<00:00, 36.49it/s]   \n",
      "100%|██████████| 47335/47335 [04:59<00:00, 158.12it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_df['describe_ls'] = clean_df['describe'].progress_apply(lambda x: process_text(x, extra_list))\n",
    "clean_df['expect_ls'] = clean_df['expect'].progress_apply(lambda x: process_text(x, extra_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bf6ed57-b725-4ec8-8b8b-808f269c113f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47335/47335 [00:08<00:00, 5681.79it/s]\n",
      "100%|██████████| 47335/47335 [00:07<00:00, 6473.29it/s]\n",
      "100%|██████████| 47335/47335 [00:08<00:00, 5637.76it/s]\n",
      "100%|██████████| 47335/47335 [00:07<00:00, 6267.54it/s]\n",
      "100%|██████████| 47335/47335 [00:08<00:00, 5416.53it/s]\n",
      "100%|██████████| 47335/47335 [00:01<00:00, 30914.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 10111.63it/s]\n",
      "100%|██████████| 47335/47335 [00:15<00:00, 3104.66it/s]\n",
      "100%|██████████| 47335/47335 [00:14<00:00, 3219.74it/s]\n",
      "100%|██████████| 47335/47335 [00:05<00:00, 8305.85it/s]\n",
      "100%|██████████| 47335/47335 [00:00<00:00, 48271.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 6054.13it/s]\n",
      "100%|██████████| 47335/47335 [00:15<00:00, 2998.11it/s]\n",
      "100%|██████████| 47335/47335 [00:15<00:00, 3130.64it/s]\n",
      "100%|██████████| 47335/47335 [01:48<00:00, 435.67it/s]\n",
      "100%|██████████| 47335/47335 [01:45<00:00, 449.97it/s]\n",
      "100%|██████████| 47335/47335 [00:05<00:00, 9380.14it/s] \n",
      "100%|██████████| 47335/47335 [00:03<00:00, 13394.63it/s]\n",
      "100%|██████████| 47335/47335 [00:09<00:00, 4812.10it/s]\n",
      "100%|██████████| 47335/47335 [00:09<00:00, 5165.60it/s]\n",
      "100%|██████████| 47335/47335 [01:48<00:00, 436.97it/s]\n",
      "100%|██████████| 47335/47335 [01:45<00:00, 446.70it/s]\n",
      "100%|██████████| 47335/47335 [00:05<00:00, 8410.15it/s] \n",
      "100%|██████████| 47335/47335 [00:03<00:00, 14097.63it/s]\n",
      "100%|██████████| 47335/47335 [00:10<00:00, 4659.87it/s]\n",
      "100%|██████████| 47335/47335 [00:09<00:00, 4989.37it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_fq_des,cat_fq_exp = ar1(clean_df, cat_df)\n",
    "cat_fq_des_10,cat_fq_exp_10 = ar1(clean_df, cat_df_10)\n",
    "cat_sm_des,cat_sm_exp = ar2(clean_df, cat_df)\n",
    "cat_sm_des_10,cat_sm_exp_10 = ar2(clean_df, cat_df_10)\n",
    "cat_kw_des,cat_kw_exp = ar3(clean_df, cat_df)\n",
    "cat_kw_des_10,cat_kw_exp_10 = ar3(clean_df, cat_df_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76700cfa-106a-4ea2-a9cd-8a63ce3dc99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47335/47335 [00:00<00:00, 109667.41it/s]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "result['label_frequence'] = cat_fq_des\n",
    "result['label_frequence_10'] = cat_fq_des_10\n",
    "result['label_similarity'] = cat_sm_des\n",
    "result['label_similarity_10'] = cat_sm_des_10\n",
    "result['label_keywords'] = cat_kw_des\n",
    "result['label_keywords_10'] = cat_kw_des_10\n",
    "final_des_label = result.progress_apply(most_frequent_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdfc42cf-b218-4ad2-9403-ecac89748278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47335/47335 [00:00<00:00, 106619.52it/s]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "result['label_frequence'] = cat_fq_exp\n",
    "result['label_frequence_10'] = cat_fq_exp_10\n",
    "result['label_similarity'] = cat_sm_exp\n",
    "result['label_similarity_10'] = cat_sm_exp_10\n",
    "result['label_keywords'] = cat_kw_exp\n",
    "result['label_keywords_10'] = cat_kw_exp_10\n",
    "final_exp_label = result.progress_apply(most_frequent_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a5ac025-84cc-41ec-af80-74afc39ae218",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['describe_label'] = final_des_label\n",
    "clean_df['expect_label'] = final_exp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f50306-92e0-4e70-86da-da2e8ea23f9f",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9be0f511-3a61-456c-8df2-650b21355f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you have hair...and teeth.<br />\\nyou like to have fun, and you know how to.<br />\\nyou were born a female.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df.id == 16440]['expect'][16440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b804776d-bec2-49f5-bfa1-d8246c4b086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you are down to earth, open minded, kind, funny, friendly, honest,\\nspontaneous, willing to play, passionate, a good listener and\\ncommunicator and soulful.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df.id == 2340]['expect'][2340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3443be25-618b-4061-976a-033b516ae8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have hair...and teeth.<br />\n",
      "you like to have fun, and you know how to.<br />\n",
      "you were born a female. Creative/Artistic\n",
      "------------------------------\n",
      "i'm just kind of looking to see who's out there. i'm not someone\n",
      "who needs to be in a relationship to feel complete. i like myself\n",
      "(though i suppose i could be more ambitious) and don't mind being\n",
      "alone. in fact, i enjoy it. that way, i always get to do what i\n",
      "want. . music (singing, occasionally composing), writing,\n",
      "design/decorating, trivia. a television/movies<br />\n",
      "music<br />\n",
      "my imagination<br />\n",
      "air<br />\n",
      "family<br />\n",
      "friends at the gym, then home alone, watching tv/movies, reading.\n",
      "------------------------------\n",
      "chill guy here, masculine, smart, fun, honest, and loyal. i'm\n",
      "looking to get to know other guys to hang out with and go from\n",
      "there. good people and good conversation are important. i'm not\n",
      "really down with pop culture. working class, tats, scruff, geeks\n",
      "and great smiles are sexy. i help people, and have fun. and that's really working for me right\n",
      "now. fixing things spicy food, my laptop, cute guys, horror, new ideas, the people i\n",
      "can rely on. drinking and hanging out with friends.\n",
      "------------------------------\n",
      "i'm pretty much a big goofy free spirit, i'm honest but can be very\n",
      "sarcastic. i love to joke around, make funny faces/sounds and make\n",
      "people laugh. i graduated from a small private college outside of\n",
      "boston and then moved out to the bay about three years ago. i love\n",
      "it here.<br />\n",
      "<br />\n",
      "i like camping and hiking, especially with my dog. he's so much fun\n",
      "to watch him run around smiling the whole time, loving all the new\n",
      "smells. i love going to shows and listening to live music. i like\n",
      "to read, paint, cook on open fires in our backyard. i live with\n",
      "three other awesome people, they're some of the best roommates i've\n",
      "ever had. i've been known to play cheese while listening to\n",
      "classical music and drinking whiskey. i def consider myself a noble\n",
      "leader of men. i'm a passionate lover and i love women of all\n",
      "kinds, i can find beauty in almost anyone.<br />\n",
      "<br />\n",
      "i spent a lot of time travel last year, so i'm excited to see where\n",
      "this year brings me. coachella was one of the best weekends of my\n",
      "entire life. i live for the festival atmosphere. living life, trying to take my time with things. too many people\n",
      "are in such a rush. i want to smell the roses.<br />\n",
      "<br />\n",
      "oh and making wax art. cooking i love cooking for other people<br />\n",
      "<br />\n",
      "listening, rolling spliffs, basketball, having fun, laughing,\n",
      "playing with my dog. playing with kids, usually they're intimidated\n",
      "by my size at first, but after a few minutes they realize i want to\n",
      "play just as much as they do and then i cant get them to leave me\n",
      "alone.<br />\n",
      "<br />\n",
      "and of course, drinking whiskey while being really tall music, cheese, burritos, my dog, yoga and my closest friends what night in the bay doesnt have the potential to be a \"friday\"?\n"
     ]
    }
   ],
   "source": [
    "match_16440 = clean_df[clean_df.describe_label == 'Creative/Artistic'].sample(n=3, replace=False, random_state=42)\n",
    "print(clean_df[clean_df.id == 16440]['expect'][16440],clean_df[clean_df.id == 16440]['expect_label'][16440])\n",
    "for i in match_16440.index:\n",
    "    print('------------------------------')\n",
    "    print(match_16440.loc[i,'describe'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2b76660-80f2-4622-833f-45c8868cdf27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are down to earth, open minded, kind, funny, friendly, honest,\n",
      "spontaneous, willing to play, passionate, a good listener and\n",
      "communicator and soulful. Party/Nightlife Lover\n",
      "------------------------------\n",
      "i've been lurking on here for a while now and browse the occasional\n",
      "profile and answer a question or 3. it makes a nice change from\n",
      "farcebook(tm)<br />\n",
      "<br />\n",
      "there are a couple of potabilities: you're browsing or otherwise\n",
      "found your way here. welcome!<br />\n",
      "<br />\n",
      "or: i've sent you a message and you came here to see if you can\n",
      "find out a bit more about me before deciding if you want to reply.\n",
      "very sensible but think for a moment how many guys put the truth\n",
      "down in these profile things?<br />\n",
      "<br />\n",
      "i've allways wanted to be a 6' 6\" rocket scientist who looks just\n",
      "like (insert name of movie star here. anyone apart from brad pitt)\n",
      "thanks to the wonders of the www i and millions of others can now\n",
      "be who we want to be. scarey isn't it?<br />\n",
      "<br />\n",
      "maybe i'll reinvent myself some time and live the rest of my life\n",
      "as my .alt? meanwhile as far as i know this is the real me and who\n",
      "is better to judge?<br />\n",
      "<br />\n",
      "i am cynical, iconoclastic, and myself i moved to the bay area a while ago and promptly became a\n",
      "work-a-holic in order to pay for my home. i do get rather a nice\n",
      "view of the beach and the city so i'm not complaining or anything.\n",
      "in fact i am becoming a bit of a curmudgeon and not entirely\n",
      "worried by that prospect.<br />\n",
      "<br />\n",
      "no wife, no kids, no drama. apparently makes me an ideal catch for\n",
      "the serial divorcee looking for somone to be the daddy to\n",
      "themselves and ther 12 children. so i don't get out much and who\n",
      "can blame me?<br />\n",
      "<br />\n",
      "i did mention earlier i was cynical didn't i? probably deep down\n",
      "somewhere i'm just a big softy. if that's true it's deep down in a\n",
      "pit in the cellar living on a diet of stale bread and rainwater. a lot of stuff. i'm ok at even more and completly suck at some.\n",
      "answering this question being the perfect example.<br />\n",
      "<br />\n",
      "if i try to make it sound interesting and exciting i start to feel\n",
      "like a show off and modesty just dosn't seem to work either.<br />\n",
      "<br />\n",
      "the best short answer i can come up with is:<br />\n",
      "most things if i try hard enough! sight, hearing, touch, taste, smell and a sense of humor reading, surfing the intra-web watching a movie or\n",
      "documentary.<br />\n",
      "<br />\n",
      "looking out of the window and trying to avoid social contact with\n",
      "actual human beings.\n",
      "------------------------------\n",
      ". . singing marvin gaye and elton john songs at the karaoke bar.\n",
      "(slight intoxication required) - family (typical response, but they really are the most important\n",
      "part of my life)<br />\n",
      "- close friends (ditto)<br />\n",
      "- a good cup of joe<br />\n",
      "- my private time (but not in a perverted sort of way :p)<br />\n",
      "- enjoying a good cup of joe with my family or close friends\n",
      "(smartass...)<br />\n",
      "- you (aww!) doing the moonwalk in my socks on the kitchen floor.\n",
      "------------------------------\n",
      "i eat breakfast.<br />\n",
      "i poop (it smells like roses, thank you for asking).<br />\n",
      "i work.<br />\n",
      "i eat salad for lunch (don't judge you women-eating-salad\n",
      "haters).<br />\n",
      "i then work some more. by working i mean talking to my workers in\n",
      "the philippines. by talking i mean asking them, \"why soo\n",
      "slowwww?\"<br />\n",
      "go bench some hipsters.<br />\n",
      "eat dinner.<br />\n",
      "gossip with friends.<br />\n",
      "drink with friends.<br />\n",
      "update my profile while drinking (can you tell? lol!)<br />\n",
      "message back non-creepers<br />\n",
      "eat cereal (lucky charms of course)<br />\n",
      "sleep like a bear duh - winning. riding horses (yeah that's code). humor<br />\n",
      "wit<br />\n",
      "innovation<br />\n",
      "progress<br />\n",
      "depth of vision<br />\n",
      "great friends<br />\n",
      "minions spilling drinks all over drunks.\n"
     ]
    }
   ],
   "source": [
    "match_2340 = clean_df[clean_df.describe_label == 'Party/Nightlife Lover'].sample(n=3, replace=False, random_state=42)\n",
    "print(clean_df[clean_df.id == 2340]['expect'][2340],clean_df[clean_df.id == 2340]['expect_label'][2340])\n",
    "for i in match_2340.index:\n",
    "    print('------------------------------')\n",
    "    print(match_2340.loc[i,'describe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "710a6e67-6f77-459e-a415-3129d038e97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you like having fun, like adventures! dont judge. and dont care\n",
      "about deep conversations or politics (thank you in advance!) Adventurous Outdoorsy\n",
      "------------------------------\n",
      "just moved to the city from chicago not too long ago so i don't\n",
      "know very many people in the area. i am looking for a fun,\n",
      "intelligent, laid back and most importantly drama free girl who i\n",
      "can explore the city with and with whom i can at least be friends\n",
      "with if nothing else ensues. i like to live life to its fullest and\n",
      "do what makes me happy and hope that you do too. i love to meet new\n",
      "people, if you want to know more, just ask :)<br />\n",
      "<br />\n",
      "i am no drama, silly, and laid back working for a startup in sv! seinfeld trivia friends/family<br />\n",
      "sunshine<br />\n",
      "staying fit<br />\n",
      "sense of humor<br />\n",
      "sports<br />\n",
      "expanding my knowledge out exploring the city!\n",
      "------------------------------\n",
      "racing down mount diablo on my bicycle is pure joy, but crashing my\n",
      "bicycle on the mountain is not. i am more cautious now in life, and\n",
      "i hate that about myself, but i'm not too cautious. i still race\n",
      "down the mountain just with my fingers lightly on the brakes.<br />\n",
      "<br />\n",
      "on a lighter note, and there is a lighter note. i like the\n",
      "accidents of life. the chance meeting of an old friend, the\n",
      "happening of a new band, or new discovery. i love the sweet smell\n",
      "of weeds and flowers on a morning hike, but i also like the with\n",
      "whiffs and riffs of music.<br />\n",
      "<br />\n",
      "i love to make people smile and laugh, but i've been told i'm no\n",
      "comedian; i'm more in the mold of an mc. i also appreciate someone\n",
      "who can make me smile. i'm moved by a measure of inner and outer\n",
      "beauty, intelligence, sensitivity and congeniality. i have a\n",
      "genuine attraction for emotional generosity and empathy. i tend to\n",
      "prefer a woman who is comfortable with herself. i think about\n",
      "meeting someone with whom i could build a nurturing relationship -\n",
      "something that enhances the good times by sharing them. when i'm not riding or crashing my bike, i work in the\n",
      "transportation field, trying to make the world safer for\n",
      "pedestrians and bicyclists. yoga has become a daily practice for\n",
      "me. i also swim and walk/hike almost daily. i'm really good talking about bridges and how to improve our\n",
      "transportation system. tabasco sauce<br />\n",
      "laughter<br />\n",
      "cool breeze on a hot night<br />\n",
      "memories<br />\n",
      "books<br />\n",
      "happiness i'm singing in safeway or dancing down my street, or playing\n",
      "monopoly in the trees. i try not to have typical friday nights.\n",
      "------------------------------\n",
      "i like to think that i am a pretty eclectic guy. i grew up here in\n",
      "the bay area. i went to school on the central coast, but found my\n",
      "way back the city about five years ago and have worked in the world\n",
      "of \"online\" with various startups ever since. i love it - it lets\n",
      "me do a bit of traveling, talk to really cool people, and be\n",
      "innovative.<br />\n",
      "<br />\n",
      "i love the city life and everything it has to offer, but i really\n",
      "need nature. i love hiking, camping, getting dirty, biking, and\n",
      "staying active. even if it's just a bike ride over the gg bridge.\n",
      "did the aids lifecycle this past june - it was amazing (life\n",
      "changing really). i used to be a competitive figure skater (past\n",
      "life) and am an eagle scout. somewhat of a contradiction, i know,\n",
      "but i feel lucky to have done both. also, obsessed with animals - i\n",
      "wish i could live on a farm in the city. i've been to over 100\n",
      "baseball games - i love my giants :)<br />\n",
      "<br />\n",
      "i also grew up wanting to be a chef, but never followed through on\n",
      "that. now i find myself in the kitchen whipping up meals for my\n",
      "friends. it's a great compromise (and i think i'm pretty good at\n",
      "it!).<br />\n",
      "<br />\n",
      "i am very grateful for the fun life i have lived already and i try\n",
      "to remember that often. i love my job, but it's not what \"i'm doing with my life.\" i like\n",
      "to travel, hang with my family, friends, have a good bottle of\n",
      "wine, hike, bike, and relax. that's life =)<br />\n",
      "<br />\n",
      "i like letting life just \"happen\" these days. tying boy scout knots, talking and blabbing on about things even\n",
      "when people stop listening to me, oh, and darts...how i am so good\n",
      "at it, i don't know, but i have never lost a round. seriously. good food, genuine friends, exercise, wine, animals, and fresh air. i like a well-deserved happy hour every now and then. other than\n",
      "that, making a feast with friends in my kitchen. i love my kitchen.\n"
     ]
    }
   ],
   "source": [
    "match_307 = clean_df[clean_df.describe_label == 'Adventurous Outdoorsy'].sample(n=3, replace=False, random_state=42)\n",
    "print(clean_df[clean_df.id == 307]['expect'][307],clean_df[clean_df.id == 307]['expect_label'][307])\n",
    "\n",
    "for i in match_307.index:\n",
    "    print('------------------------------')\n",
    "    print(match_307.loc[i,'describe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a5b6f9f-c062-4446-a2da-14caed5a8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you know where to get good food and willing to share intelligence Foodie/Culinary\n",
      "------------------------------\n",
      "i value joy, honesty, and an appreciation of the beauty in the\n",
      "world and in people. i try to be open to the experiences of the\n",
      "world that others have to share, but i'm not always good at\n",
      "listening. i want to do something good and important with my life,\n",
      "and so far i've had a few good tries, but i'm not quite sure at the\n",
      "moment of what the 'next big thing' will be for me. i have spent a\n",
      "lot of time in school and while i'm always a sucker for gobbling up\n",
      "new ideas, there will be no more school in my future (unless i'm\n",
      "teaching it!)<br />\n",
      "<br />\n",
      "i know that i want to share my life with someone. someone who has a\n",
      "good heart and positive energy. i think it will probably be\n",
      "important that we have a fair number of activity-type interests in\n",
      "common, but not every single interest has gotta be the same.<br />\n",
      "<br />\n",
      "ah, what do i do? well, i'm a scientist for now, and i'm aspiring\n",
      "to be a teacher.<br />\n",
      "<br />\n",
      "i am heartfelt, proud, and observant enjoying myself, my body, my health. cooking yummy vegetable\n",
      "dishes. biking (road, but i'm open to mountain), hiking, camping,\n",
      "playing music again...living one day at a time and appreciating the\n",
      "great moments that come every day! 1) figuring things out. 2) letting people in. my friends, my heart, biking, amazing food, nature, living near a\n",
      "city cleaning the house so i can enjoy the rest of the weekend, cooking\n",
      "a nice dinner, and basically taking care of my self. you know, once\n",
      "in a while i'd like to be out on a date!\n",
      "------------------------------\n",
      "if i had to describe myself in two words i would fashion two giant\n",
      "new words from all the letters of the alphabet. i would then wield\n",
      "these words like great rocks in a rock wielding contest of some\n",
      "sort and hurl them against the fortress walls. this is because my\n",
      "spirit animal is a lobster and lacks the ability of flight. stand up, work as a comedy writer/social media bs.<br />\n",
      "<br />\n",
      "live in a co-op in south berkeley. i love it. it's all the pros of\n",
      "hippy radicalism without any of the heavy handed bullshit.<br />\n",
      "<br />\n",
      "please enjoy my comedy humor: www.youtube.com/watch?v=frqn5ngghzq . friends<br />\n",
      "work<br />\n",
      "food spice<br />\n",
      "good books<br />\n",
      "something<br />\n",
      "something standing up\n",
      "------------------------------\n",
      "(non) artist<br />\n",
      "human conditions<br />\n",
      "metaphorical arson pennsylvania blood running through the golden state. reinforcing\n",
      "stereotypes by spending most of my days walking with canine,\n",
      "pedalling a heavy bicycle into the hills and spending way too many\n",
      "hours in the kitchen. mildly obsessing over coffee, camping spots,\n",
      "unvisited thrift stores. making beer and getting it all over the\n",
      "floor. paying the rent in fruit. some form of physical activity, on\n",
      "the daily. making music and getting down. learning how to make food\n",
      "grow/finding already-growing food. singing &amp; dancing in as many\n",
      "places worldwide as my body &amp; finances allow. letters. grandiose meals. fixing things. playing certain styles of\n",
      "music. talking shit. talking about shaving (beard is an\n",
      "experiment). resourcefulness. really short camping trips. really\n",
      "long camping trips. sarcasm. lists. kafka-esque cynicism.\n",
      "celebration. the right people<br />\n",
      "a sharp knife in the kitchen<br />\n",
      "the bicycle<br />\n",
      "weather<br />\n",
      "tunes<br />\n",
      "forthcoming libations one thing or the other.\n"
     ]
    }
   ],
   "source": [
    "match_629 = clean_df[clean_df.describe_label == 'Foodie/Culinary'].sample(n=3, replace=False, random_state=32)\n",
    "print(clean_df[clean_df.id == 629]['expect'][629],clean_df[clean_df.id == 629]['expect_label'][629])\n",
    "\n",
    "for i in match_629.index:\n",
    "    print('------------------------------')\n",
    "    print(match_629.loc[i,'describe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74d9ba97-82d8-4e9f-b313-40a51da76aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.to_csv('clean_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bd958-94d3-48bc-814e-d7d15e6db1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
